{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8F04vYfcov29IZDOWq64Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/jeffvestal/869036b99340783d262685783a3b6990/developer-genai-group2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nFbQGw2POViM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "CZO8krnZMLZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Libraries"
      ],
      "metadata": {
        "id": "gM0zjk2VR2OR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit openai elasticsearch\n",
        "!npm install localtunnel --loglevel=error\n",
        "import getpass, os"
      ],
      "metadata": {
        "id": "Ln-8SRvAI-jS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel --loglevel=error"
      ],
      "metadata": {
        "id": "02NstyujMogn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Elastic Cloud Credentials\n",
        "Cloud ID -> Elastic Cloud Admin console\n",
        "\n",
        "Elasticsearch API Keys - Generate in Kibana -> Stack Mgt\n"
      ],
      "metadata": {
        "id": "av3OYD76NRaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['es_cloud_id'] = getpass.getpass('Enter Elastic Cloud ID:  ')\n",
        "os.environ['es_api_id'] = getpass.getpass('Enter cluster API key ID:  ')\n",
        "os.environ['es_api_key'] = getpass.getpass('Enter cluster API key:  ')"
      ],
      "metadata": {
        "id": "ZIhMWJw0NIhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from elasticsearch import Elasticsearch\n",
        "es_cloud_id = os.environ['es_cloud_id']\n",
        "es_api_id = os.environ['es_api_id']\n",
        "es_api_key = os.environ['es_api_key']\n",
        "es = Elasticsearch(cloud_id=es_cloud_id,\n",
        "                   api_key=(es_api_id, es_api_key)\n",
        "                   )\n",
        "\n",
        "print(es.info()) # should return cluster info"
      ],
      "metadata": {
        "id": "2Y131IKGVI15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI / Azure OpenAI Credentials"
      ],
      "metadata": {
        "id": "KOmks8SrNIhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model = getpass.getpass('Enter model: ')\n",
        "os.environ['model'] = \"gpt-35-turbo\"\n",
        "\n",
        "## Getpass version of ENVs for colab\n",
        "#openai.verify_ssl_certs = False\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = getpass.getpass('Enter OPENAI_API_KEY: ')\n",
        "os.environ['OPENAI_API_TYPE'] = getpass.getpass('Enter OPENAI_API_TYPE: ')\n",
        "os.environ['OPENAI_API_BASE'] = getpass.getpass('Enter OPENAI_API_BASE: ')\n",
        "os.environ['OPENAI_API_VERSION'] = getpass.getpass('Enter OPENAI_API_VERSION: ')\n",
        "os.environ['OPENAI_API_ENGINE'] = getpass.getpass('Enter OPENAI_API_ENGINE: ')"
      ],
      "metadata": {
        "id": "xKh2rBhVNIhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Script\n",
        "Running this cell will write a file named `app.py`"
      ],
      "metadata": {
        "id": "t7RmurdZNPg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import os\n",
        "import streamlit as st\n",
        "import openai\n",
        "from elasticsearch import Elasticsearch\n",
        "from string import Template\n",
        "\n",
        "\n",
        "# Set Required Environment Variables\n",
        "\n",
        "# Elastic Cloud\n",
        "es_cloud_id = os.environ['es_cloud_id']\n",
        "es_api_id = os.environ['es_api_id']\n",
        "es_api_key = os.environ['es_api_key']\n",
        "\n",
        "\n",
        "# OpenAI\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']\n",
        "model = os.environ['model']\n",
        "\n",
        "#Below is for Azure OpenAI\n",
        "openai.api_type = os.environ['OPENAI_API_TYPE']\n",
        "openai.api_base = os.environ['OPENAI_API_BASE']\n",
        "openai.api_version = os.environ['OPENAI_API_VERSION']\n",
        "engine = os.environ['OPENAI_API_ENGINE']\n",
        "openai.verify_ssl_certs = False\n",
        "\n",
        "\n",
        "# TODO show openai direct env\n",
        "\n",
        "\n",
        "\n",
        "def es_connect(es_cloud_id, es_api_id, es_api_key):\n",
        "    '''Connect to Elastic Cloud cluster'''\n",
        "    es = Elasticsearch(cloud_id=es_cloud_id,\n",
        "                   api_key=(es_api_id, es_api_key)\n",
        "                   )\n",
        "    print(es.info()) # should return cluster info\n",
        "    return es\n",
        "\n",
        "def search_bm25(query_text, es):\n",
        "    '''Using ELSER -\n",
        "       Search ElasticSearch index and return body and URL of the result'''\n",
        "\n",
        "    # BM25 Query\n",
        "    query = {\n",
        "        \"match\": {\n",
        "            \"chunk\": query_text\n",
        "        }\n",
        "    }\n",
        "\n",
        "    fields= [\n",
        "        \"title\",\n",
        "        \"url\",\n",
        "        \"position\",\n",
        "        \"body_content\"\n",
        "      ]\n",
        "\n",
        "    collapse= {\n",
        "    \"field\": \"title.enum\"\n",
        "    }\n",
        "\n",
        "    index = 'chunker'\n",
        "    resp = es.search(index=index,\n",
        "                     query=query,\n",
        "                     fields=fields,\n",
        "                     collapse=collapse,\n",
        "                     size=1,\n",
        "                     source=False)\n",
        "\n",
        "    body = resp['hits']['hits'][0]['fields']['body_content'][0]\n",
        "    url = resp['hits']['hits'][0]['fields']['url'][0]\n",
        "\n",
        "    return body, url\n",
        "\n",
        "\n",
        "def search_elser(query_text, es):\n",
        "    '''Using ELSER -\n",
        "       Search ElasticSearch index and return body and URL of the result'''\n",
        "\n",
        "    # ELSER Query\n",
        "    query = {\n",
        "    \"text_expansion\": {\n",
        "      \"ml.inference.chunk_expanded.tokens\": {\n",
        "        \"model_id\": \".elser_model_1\",\n",
        "        \"model_text\": query_text\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "    fields= [\n",
        "        \"title\",\n",
        "        \"url\",\n",
        "        \"position\",\n",
        "        \"body_content\"\n",
        "      ]\n",
        "\n",
        "    collapse= {\n",
        "    \"field\": \"title.enum\"\n",
        "    }\n",
        "\n",
        "    index = 'chunker'\n",
        "    resp = es.search(index=index,\n",
        "                     query=query,\n",
        "                     fields=fields,\n",
        "                     collapse=collapse,\n",
        "                     size=1,\n",
        "                     source=False)\n",
        "\n",
        "    body = resp['hits']['hits'][0]['fields']['body_content'][0]\n",
        "    url = resp['hits']['hits'][0]['fields']['url'][0]\n",
        "\n",
        "    return body, url\n",
        "\n",
        "def search_knn(query_text, es):\n",
        "    # Elasticsearch query (BM25) and kNN configuration for rrf hybrid search\n",
        "\n",
        "    query = {\n",
        "        \"bool\": {\n",
        "            \"must\": [{\n",
        "                \"match\": {\n",
        "                    \"title\": {\n",
        "                        \"query\": query_text\n",
        "                    }\n",
        "                }\n",
        "            }],\n",
        "            \"filter\": [{\n",
        "              \"term\": {\n",
        "                \"url_path_dir3\": \"elasticsearch\"\n",
        "              }\n",
        "            }]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    knn = [\n",
        "    {\n",
        "      \"field\": \"chunk-vector\",\n",
        "      \"k\": 10,\n",
        "      \"num_candidates\": 10,\n",
        "      \"filter\": {\n",
        "        \"bool\": {\n",
        "          \"filter\": [\n",
        "            {\n",
        "              \"range\": {\n",
        "                \"chunklength\": {\n",
        "                  \"gte\": 0\n",
        "                }\n",
        "              }\n",
        "            },\n",
        "            {\n",
        "              \"term\": {\n",
        "                \"url_path_dir3\": \"elasticsearch\"\n",
        "              }\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "      },\n",
        "      \"query_vector_builder\": {\n",
        "        \"text_embedding\": {\n",
        "          \"model_id\": \"sentence-transformers__msmarco-minilm-l-12-v3\",\n",
        "          \"model_text\": query_text\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "\n",
        "    rank = {\n",
        "       \"rrf\": {\n",
        "       }\n",
        "   }\n",
        "\n",
        "#    collapse = {\n",
        "#    \"field\": \"title.enum\"\n",
        "#    }\n",
        "\n",
        "    fields= [\n",
        "        \"title\",\n",
        "        \"url\",\n",
        "        \"position\",\n",
        "        \"url_path_dir3\",\n",
        "        \"body_content\"\n",
        "      ]\n",
        "\n",
        "    index = 'chunker'\n",
        "    resp = es.search(index=index,\n",
        "                     query=query,\n",
        "                     knn=knn,\n",
        "                     rank=rank,\n",
        "#                     collapse=collapse,\n",
        "                     fields=fields,\n",
        "                     size=10,\n",
        "                     source=False)\n",
        "\n",
        "    body = resp['hits']['hits'][0]['fields']['body_content'][0]\n",
        "    url = resp['hits']['hits'][0]['fields']['url'][0]\n",
        "\n",
        "    return body, url\n",
        "\n",
        "def truncate_text(text, max_tokens):\n",
        "    tokens = text.split()\n",
        "    if len(tokens) <= max_tokens:\n",
        "        return text\n",
        "\n",
        "    return ' '.join(tokens[:max_tokens])\n",
        "\n",
        "def chat_gpt(prompt, model=\"gpt-3.5-turbo\", max_tokens=1024, max_context_tokens=4000, safety_margin=5, sys_content=None):\n",
        "    '''# Generate a response from ChatGPT based on the given prompt'''\n",
        "\n",
        "    # Truncate the prompt content to fit within the model's context length\n",
        "    truncated_prompt = truncate_text(prompt, max_context_tokens - max_tokens - safety_margin)\n",
        "\n",
        "    response = openai.ChatCompletion.create(engine=engine,\n",
        "                                            temperature=0,\n",
        "                                            messages=[{\"role\": \"system\", \"content\": sys_content},\n",
        "                                                      {\"role\": \"user\", \"content\": truncated_prompt}]\n",
        "                                           )\n",
        "\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "\n",
        "def toLLM(resp,\n",
        "          url,\n",
        "          usr_prompt,\n",
        "          sys_prompt,\n",
        "          neg_resp,\n",
        "          show_prompt):\n",
        "\n",
        "    prompt_template = Template(usr_prompt)\n",
        "    prompt_formatted = prompt_template.substitute(query=query, resp=resp, negResponse=negResponse)\n",
        "    answer = chat_gpt(prompt_formatted, sys_content=sys_prompt)\n",
        "\n",
        "    st.header('Response from LLM')\n",
        "    # Display response from LLM\n",
        "    st.markdown(answer.strip())\n",
        "\n",
        "    # We don't need to return a reference URL if it wasn't useful\n",
        "    if not negResponse in answer:\n",
        "        st.write(url)\n",
        "\n",
        "    # Display full prompt if checkbox was selected\n",
        "    if show_prompt:\n",
        "        st.divider()\n",
        "        st.subheader('Full prompt sent to LLM')\n",
        "        prompt_formatted\n",
        "\n",
        "\n",
        "\n",
        "# MAIN\n",
        "es = es_connect(es_cloud_id, es_api_id, es_api_key)\n",
        "\n",
        "# Prompt Defaults\n",
        "prompt_default = \"\"\"Answer this question: $query\n",
        "Using only the information from this Elastic Doc: $resp\n",
        "Format the answer in complete markdown code format\n",
        "If the answer is not contained in the supplied doc reply '$negResponse' and nothing else\"\"\"\n",
        "\n",
        "system_default = 'You are a helpful assistant.'\n",
        "neg_default = \"I'm unable to answer the question based on the information I have from Elastic Docs.\"\n",
        "\n",
        "\n",
        "# Main chat form\n",
        "st.title(\"ElasticDocs GPT\")\n",
        "\n",
        "with st.form(\"chat_form\"):\n",
        "    query = st.text_input(\"Ask the Elastic Documentation a question: \")\n",
        "\n",
        "    # Inputs for system and User prompt override\n",
        "    sys_prompt = st.text_area(\"create an alernative system prompt\", placeholder=system_default, value=system_default)\n",
        "    usr_prompt = st.text_area(\"create an alternative user prompt required -> \\$query, \\$resp, \\$negResponse\",\n",
        "                             placeholder=prompt_default, value=prompt_default )\n",
        "\n",
        "    # Default Response when criteria are not met\n",
        "    negResponse = st.text_area(\"Create an alternative negative response\", placeholder = neg_default, value=neg_default)\n",
        "\n",
        "    show_full_prompt = st.checkbox('Show Full Prompt Sent to LLM')\n",
        "\n",
        "    # Query Submit Buttons\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    with col1:\n",
        "        bm25_button = st.form_submit_button(\"Use BM25\")\n",
        "    with col2:\n",
        "        knn_button = st.form_submit_button(\"Use kNN\")\n",
        "    with col3:\n",
        "        elser_button = st.form_submit_button(\"Use ELSER\")\n",
        "\n",
        "\n",
        "\n",
        "if elser_button:\n",
        "    resp, url = search_elser(query, es) # run ELSER query\n",
        "    toLLM(resp, url, usr_prompt, sys_prompt, negResponse, show_full_prompt)\n",
        "\n",
        "if knn_button:\n",
        "    resp, url = search_knn(query, es) # run kNN hybrid query\n",
        "    toLLM(resp, url, usr_prompt, sys_prompt, negResponse, show_full_prompt)\n",
        "\n",
        "if bm25_button:\n",
        "    resp, url = search_bm25(query, es) # run kNN hybrid query\n",
        "    toLLM(resp, url, usr_prompt, sys_prompt, negResponse, show_full_prompt)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Wc02OlkpOSd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streamlit\n",
        "Running this cell will start local tunnel and generate a random URL\n",
        "\n",
        "Copy the IP address on the first line then open the generated URL and paste it in the input box \"Endpoint IP\"\n",
        "\n",
        "This will then start the Streamlit app"
      ],
      "metadata": {
        "id": "Wu0KfS0ESf6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "id": "cHIHFID3NBXa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
