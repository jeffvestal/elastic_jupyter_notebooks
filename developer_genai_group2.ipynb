{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPbIymtCw87UtjcA/6Biyb+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffvestal/elastic_jupyter_notebooks/blob/main/developer_genai_group2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nFbQGw2POViM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "CZO8krnZMLZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Libraries"
      ],
      "metadata": {
        "id": "gM0zjk2VR2OR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit openai elasticsearch\n",
        "import getpass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ln-8SRvAI-jS",
        "outputId": "0adeec43-0b21-418c-b757-4fc1c972c65b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.9/393.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel --loglevel=error"
      ],
      "metadata": {
        "id": "02NstyujMogn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Elastic Cloud Credentials\n",
        "Cloud ID -> Elastic Cloud Admin console\n",
        "\n",
        "Elasticsearch API Keys - Generate in Kibana -> Stack Mgt\n"
      ],
      "metadata": {
        "id": "av3OYD76NRaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test = getpass.getpass('test: ')\n",
        "#os.environ['test'] = test\n",
        "#print(os.getenv('test'))\n",
        "\n",
        "os.environ['es_cloud_id'] = getpass.getpass('Enter Elastic Cloud ID:  ')\n",
        "os.environ['es_api_id'] = getpass.getpass('Enter cluster API key ID:  ')\n",
        "os.environ['es_api_key'] = getpass.getpass('Enter cluster API key:  ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIhMWJw0NIhc",
        "outputId": "74db2455-f78a-40d9-9031-031f5c2ecff8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Elastic Cloud ID:  ··········\n",
            "Enter cluster API key ID:  ··········\n",
            "Enter cluster API key:  ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI / Azure OpenAI Credentials"
      ],
      "metadata": {
        "id": "KOmks8SrNIhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model = getpass.getpass('Enter model: ')\n",
        "os.environ['model'] = \"gpt-35-turbo\"\n",
        "\n",
        "## Getpass version of ENVs for colab\n",
        "#openai.verify_ssl_certs = False\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = getpass.getpass('Enter OPENAI_API_KEY: ')\n",
        "os.environ['OPENAI_API_TYPE'] = getpass.getpass('Enter OPENAI_API_TYPE: ')\n",
        "os.environ['OPENAI_API_BASE'] = getpass.getpass('Enter OPENAI_API_BASE: ')\n",
        "os.environ['OPENAI_API_VERSION'] = getpass.getpass('Enter OPENAI_API_VERSION: ')\n",
        "os.environ['OPENAI_API_ENGINE'] = getpass.getpass('Enter OPENAI_API_ENGINE: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKh2rBhVNIhc",
        "outputId": "3726fbb4-09de-4aab-938a-e7bbbb0d71bd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter OPENAI_API_KEY: ··········\n",
            "Enter OPENAI_API_TYPE: ··········\n",
            "Enter OPENAI_API_BASE: ··········\n",
            "Enter OPENAI_API_VERSION: ··········\n",
            "Enter OPENAI_API_ENGINE: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Script\n",
        "Running this cell will write a file named `app.py`"
      ],
      "metadata": {
        "id": "t7RmurdZNPg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import os\n",
        "import streamlit as st\n",
        "import openai\n",
        "from elasticsearch import Elasticsearch\n",
        "from string import Template\n",
        "\n",
        "\n",
        "# Set Required Environment Variables\n",
        "\n",
        "# Elastic Cloud\n",
        "es_cloud_id = os.environ['es_cloud_id']\n",
        "es_api_id = os.environ['es_api_id']\n",
        "es_api_key = os.environ['es_api_key']\n",
        "\n",
        "\n",
        "# OpenAI\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']\n",
        "model = os.environ['model']\n",
        "\n",
        "#Below is for Azure OpenAI\n",
        "openai.api_type = os.environ['OPENAI_API_TYPE']\n",
        "openai.api_base = os.environ['OPENAI_API_BASE']\n",
        "openai.api_version = os.environ['OPENAI_API_VERSION']\n",
        "engine = os.environ['OPENAI_API_ENGINE']\n",
        "openai.verify_ssl_certs = False\n",
        "\n",
        "\n",
        "# TODO show openai direct env\n",
        "\n",
        "\n",
        "\n",
        "def es_connect(es_cloud_id, es_api_id, es_api_key):\n",
        "    '''Connect to Elastic Cloud cluster'''\n",
        "    es = Elasticsearch(cloud_id=es_cloud_id,\n",
        "                   api_key=(es_api_id, es_api_key)\n",
        "                   )\n",
        "    print(es.info()) # should return cluster info\n",
        "    return es\n",
        "\n",
        "def search_bm25(query_text, es):\n",
        "    '''Using ELSER -\n",
        "       Search ElasticSearch index and return body and URL of the result'''\n",
        "\n",
        "    # BM25 Query\n",
        "    query = {\n",
        "        \"match\": {\n",
        "            \"chunk\": query_text\n",
        "        }\n",
        "    }\n",
        "\n",
        "    fields= [\n",
        "        \"title\",\n",
        "        \"url\",\n",
        "        \"position\",\n",
        "        \"body_content\"\n",
        "      ]\n",
        "\n",
        "    collapse= {\n",
        "    \"field\": \"title.enum\"\n",
        "    }\n",
        "\n",
        "    index = 'chunker'\n",
        "    resp = es.search(index=index,\n",
        "                     query=query,\n",
        "                     fields=fields,\n",
        "                     collapse=collapse,\n",
        "                     size=1,\n",
        "                     source=False)\n",
        "\n",
        "    body = resp['hits']['hits'][0]['fields']['body_content'][0]\n",
        "    url = resp['hits']['hits'][0]['fields']['url'][0]\n",
        "\n",
        "    return body, url\n",
        "\n",
        "\n",
        "def search_elser(query_text, es):\n",
        "    '''Using ELSER -\n",
        "       Search ElasticSearch index and return body and URL of the result'''\n",
        "\n",
        "    # ELSER Query\n",
        "    query = {\n",
        "    \"text_expansion\": {\n",
        "      \"ml.inference.chunk_expanded.tokens\": {\n",
        "        \"model_id\": \".elser_model_1\",\n",
        "        \"model_text\": query_text\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "    fields= [\n",
        "        \"title\",\n",
        "        \"url\",\n",
        "        \"position\",\n",
        "        \"body_content\"\n",
        "      ]\n",
        "\n",
        "    collapse= {\n",
        "    \"field\": \"title.enum\"\n",
        "    }\n",
        "\n",
        "    index = 'chunker'\n",
        "    resp = es.search(index=index,\n",
        "                     query=query,\n",
        "                     fields=fields,\n",
        "                     collapse=collapse,\n",
        "                     size=1,\n",
        "                     source=False)\n",
        "\n",
        "    body = resp['hits']['hits'][0]['fields']['body_content'][0]\n",
        "    url = resp['hits']['hits'][0]['fields']['url'][0]\n",
        "\n",
        "    return body, url\n",
        "\n",
        "def search_knn(query_text, es):\n",
        "    # Elasticsearch query (BM25) and kNN configuration for rrf hybrid search\n",
        "\n",
        "    query = {\n",
        "        \"bool\": {\n",
        "            \"must\": [{\n",
        "                \"match\": {\n",
        "                    \"title\": {\n",
        "                        \"query\": query_text\n",
        "                    }\n",
        "                }\n",
        "            }],\n",
        "            \"filter\": [{\n",
        "              \"term\": {\n",
        "                \"url_path_dir3\": \"elasticsearch\"\n",
        "              }\n",
        "            }]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    knn = [\n",
        "    {\n",
        "      \"field\": \"chunk-vector\",\n",
        "      \"k\": 10,\n",
        "      \"num_candidates\": 10,\n",
        "      \"filter\": {\n",
        "        \"bool\": {\n",
        "          \"filter\": [\n",
        "            {\n",
        "              \"range\": {\n",
        "                \"chunklength\": {\n",
        "                  \"gte\": 0\n",
        "                }\n",
        "              }\n",
        "            },\n",
        "            {\n",
        "              \"term\": {\n",
        "                \"url_path_dir3\": \"elasticsearch\"\n",
        "              }\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "      },\n",
        "      \"query_vector_builder\": {\n",
        "        \"text_embedding\": {\n",
        "          \"model_id\": \"sentence-transformers__msmarco-minilm-l-12-v3\",\n",
        "          \"model_text\": query_text\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "\n",
        "    rank = {\n",
        "       \"rrf\": {\n",
        "       }\n",
        "   }\n",
        "\n",
        "#    collapse = {\n",
        "#    \"field\": \"title.enum\"\n",
        "#    }\n",
        "\n",
        "    fields= [\n",
        "        \"title\",\n",
        "        \"url\",\n",
        "        \"position\",\n",
        "        \"url_path_dir3\",\n",
        "        \"body_content\"\n",
        "      ]\n",
        "\n",
        "    index = 'chunker'\n",
        "    resp = es.search(index=index,\n",
        "                     query=query,\n",
        "                     knn=knn,\n",
        "                     rank=rank,\n",
        "#                     collapse=collapse,\n",
        "                     fields=fields,\n",
        "                     size=10,\n",
        "                     source=False)\n",
        "\n",
        "    body = resp['hits']['hits'][0]['fields']['body_content'][0]\n",
        "    url = resp['hits']['hits'][0]['fields']['url'][0]\n",
        "\n",
        "    return body, url\n",
        "\n",
        "def truncate_text(text, max_tokens):\n",
        "    tokens = text.split()\n",
        "    if len(tokens) <= max_tokens:\n",
        "        return text\n",
        "\n",
        "    return ' '.join(tokens[:max_tokens])\n",
        "\n",
        "def chat_gpt(prompt, model=\"gpt-3.5-turbo\", max_tokens=1024, max_context_tokens=4000, safety_margin=5, sys_content=None):\n",
        "    '''# Generate a response from ChatGPT based on the given prompt'''\n",
        "\n",
        "    # Truncate the prompt content to fit within the model's context length\n",
        "    truncated_prompt = truncate_text(prompt, max_context_tokens - max_tokens - safety_margin)\n",
        "\n",
        "    response = openai.ChatCompletion.create(engine=engine,\n",
        "                                            temperature=0,\n",
        "                                            messages=[{\"role\": \"system\", \"content\": sys_content},\n",
        "                                                      {\"role\": \"user\", \"content\": truncated_prompt}]\n",
        "                                           )\n",
        "\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "\n",
        "def toLLM(resp,\n",
        "          url,\n",
        "          usr_prompt,\n",
        "          sys_prompt,\n",
        "          neg_resp):\n",
        "\n",
        "    prompt_template = Template(usr_prompt)\n",
        "    prompt_formatted = prompt_template.substitute(query=query, resp=resp, negResponse=negResponse)\n",
        "    prompt_formatted\n",
        "    answer = chat_gpt(prompt_formatted, sys_content=sys_prompt)\n",
        "\n",
        "    # We don't need to return a reference URL if it wasn't useful\n",
        "    if negResponse in answer:\n",
        "        st.write(f\"ChatGPT: {answer.strip()}\")\n",
        "    else:\n",
        "        st.write(f\"ChatGPT: {answer.strip()}\\n\\nDocs: {url}\")\n",
        "\n",
        "\n",
        "# MAIN\n",
        "es = es_connect(es_cloud_id, es_api_id, es_api_key)\n",
        "\n",
        "# Prompt Defaults\n",
        "prompt_default = \"\"\"Answer this question: $query\n",
        "Using only the information from this Elastic Doc: $resp\n",
        "If the answer is not contained in the supplied doc reply '$negResponse' and nothing else\"\"\"\n",
        "\n",
        "system_default = 'You are a helpful assistant.'\n",
        "neg_default = \"I'm unable to answer the question based on the information I have from Elastic Docs.\"\n",
        "\n",
        "\n",
        "# Main chat form\n",
        "st.title(\"ElasticDocs GPT\")\n",
        "\n",
        "with st.form(\"chat_form\"):\n",
        "    query = st.text_input(\"You: \")\n",
        "\n",
        "    # Inputs for system and User prompt override\n",
        "    sys_prompt = st.text_area(\"create an alernative system prompt\", placeholder=system_default, value=system_default)\n",
        "    usr_prompt = st.text_area(\"create an alternative user prompt required -> \\$query, \\$resp, \\$negResponse\",\n",
        "                             placeholder=prompt_default, value=prompt_default )\n",
        "\n",
        "    # Default Response when criteria are not met\n",
        "    negResponse = st.text_area(\"Create an alternative negative response\", placeholder = neg_default, value=neg_default)\n",
        "\n",
        "    # Query Submit Buttons\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    with col1:\n",
        "        bm25_button = st.form_submit_button(\"Use BM25\")\n",
        "    with col2:\n",
        "        elser_button = st.form_submit_button(\"Use ELSER\")\n",
        "    with col3:\n",
        "        knn_button = st.form_submit_button(\"Use kNN\")\n",
        "\n",
        "\n",
        "\n",
        "if elser_button:\n",
        "    resp, url = search_elser(query, es) # run ELSER query\n",
        "    toLLM(resp, url, usr_prompt, sys_prompt, negResponse)\n",
        "\n",
        "if knn_button:\n",
        "    resp, url = search_knn(query, es) # run kNN hybrid query\n",
        "    toLLM(resp, url, usr_prompt, sys_prompt, negResponse)\n",
        "\n",
        "if bm25_button:\n",
        "    resp, url = search_bm25(query, es) # run kNN hybrid query\n",
        "    toLLM(resp, url, usr_prompt, sys_prompt, negResponse)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wc02OlkpOSd2",
        "outputId": "3b5bd289-3d1f-4e47-f35c-637a5b668028"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streamlit\n",
        "Running this cell will start local tunnel and generate a random URL\n",
        "\n",
        "Copy the IP address on the first line then open the generated URL and paste it in the input box \"Endpoint IP\"\n",
        "\n",
        "This will then start the Streamlit app"
      ],
      "metadata": {
        "id": "Wu0KfS0ESf6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHIHFID3NBXa",
        "outputId": "24034b46-8715-4ebb-c934-5c1ec89a856a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.138.238.29\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 3.144s\n",
            "your url is: https://every-shrimps-open.loca.lt\n"
          ]
        }
      ]
    }
  ]
}